{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "!./download_data.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from math import log, exp, pow\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(file_name):\n",
    "    \"\"\"Create one long sequence of the whole corpus\"\"\"\n",
    "    with open(file_name, \"r\") as in_file:\n",
    "        corpus = in_file.read()\n",
    "    return corpus.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ngrams(sequence, n=2):\n",
    "    \"\"\"Generate n-grams from a given iterable.\"\"\"\n",
    "    ngrams = zip(*[sequence[i:] for i in range(n)])\n",
    "    return [' '.join(ngram) for ngram in ngrams]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Probability of a word\n",
    "\n",
    "When we don't have a context word the probability of a word $w$ is the number of times it occurs normalized by cummulative sum of the frequencies of all the words in our Vocabulary $V$.\n",
    "\n",
    "$$ P(w) = \\frac{Count(w)}{\\sum_{w_i \\ in V} Count(w_i)}$$\n",
    "\n",
    "The probability of a word $w_i$ given the previous word $w_{i-1}$, $P(w_i | w_{i-1})$, is the number of times the word $w_i$ appears after the word $w_{i-1}$ normalized by the total number of times any word appears after word $w_{i-1}$. This is given by:\n",
    "\n",
    "$$ P(w_i | w_{i - 1}) = \\frac{Count(w_{i-1}, w_i)}{\\sum_{w \\in V} Count(w_{n-1}, w)}$$\n",
    "\n",
    "The denominator is equivalent to the count of the word $w_{i - 1}$ because any time the word appears alone it is part of a bigram. This means the probabibility can be calculated with:\n",
    "\n",
    "$$ P(w_i | w_{i - 1}) = \\frac{Count(w_{i-1}, w_i)}{Count(w_{i-1})} $$\n",
    "\n",
    "## Smoothing\n",
    "\n",
    "A problem with using counts is that our counts are sparse. There are many valid bigrams that we never see (and therefore have a count of $0$). This means their probability is $0$. This means that if a word in a sequence has a $0$ probability word the probability of the whole sequence is $0$. We need a way to assign some probability to unseen bigrams. \n",
    "\n",
    "Here we use Laplace (or add-one) smoothing. We add one to all counts so now words with a count of $0$ won't have a zero probability. Doing this add means that we no longer have a valid probability distribution. We added an extra observation to all words so we need to add $|V|$ (the size of our vocabulary) extra observations to our denominator. This means probabilities are calculated like so:\n",
    "\n",
    "$$ P(w_i | w_{i - 1}) = \\frac{Count(w_{i-1}, w_i) + 1}{Count(w_{i-1}) + |V|} $$\n",
    "\n",
    "## Probability of a Sentence\n",
    "\n",
    "Probabilities of sequences are calculated with the chain rule of probability\n",
    "\n",
    "$$ P(W) = P(w_1, w_2, ... , w_n) = \\prod_{k=1}^{n} P(w_k | w_{1}, w_{2}, ..., w_{k-1})$$\n",
    "\n",
    "Using a Bigram Language model we approximate the probability if a sequence with\n",
    "\n",
    "$$ P(W) = P(w_1, w_2, ... , w_n) \\approx \\prod_{k=1}^{n} P(w_k | w_{k-1})$$\n",
    "\n",
    "This calculation in done in log space to prevent underflows.\n",
    "\n",
    "$$ \\log P(W) = \\log P(w_1, w_2, ... , w_n) \\approx \\sum_{k=1}^{n} \\log P(w_k | w_{k-1}) \\\\\n",
    "    P(w) = e^{\\log P(W)}$$\n",
    "    \n",
    "## Perplexity\n",
    "\n",
    "Perplexity is the standard evaluation metric of language models. Perplexity if defined as the inverse probability of the test set (normalized by the number of words in the test set $N$). \n",
    "\n",
    "$$ Perplexity(W) = P(w_1, w_2, ... , w_N)^{\\frac{-1}{N}}$$\n",
    "\n",
    "Again, to avoid underflow this is calulated in log space\n",
    "\n",
    "$$ \\log Perplexity(W) = \\frac{-1}{N} * \\log P(W) \\\\\n",
    "   Perplexity(W) = e^{\\log Perplexity(W)}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LanguageModel:\n",
    "    \"\"\"A Bigram Language model.\"\"\"\n",
    "    \n",
    "    def __init__(self, train_corpus):\n",
    "        self.uni = Counter(train_corpus)\n",
    "        self.bi = Counter(ngrams(train_corpus))\n",
    "        self.vocab_size = len(self.uni)\n",
    "        self.uni_sum = sum(self.uni.values())\n",
    "        \n",
    "    def generate_probability(self, word, context=None):\n",
    "        \"\"\"Generate the probability with LaPlace smoothing.\"\"\"\n",
    "        if context is None:\n",
    "            return (self.uni[word] + 1) / (self.uni_sum + self.vocab_size)\n",
    "        else:\n",
    "            return (self.bi[context + ' ' + word] + 1) / (self.uni[context] + self.vocab_size) \n",
    "        \n",
    "    def _generate_distribution(self, context):\n",
    "        \"\"\"Generate the probability distribution of next word given the context.\"\"\"\n",
    "        dist = []\n",
    "        for word in self.uni:\n",
    "            dist.append((word, self.generate_probability(word, context)))\n",
    "        return dist\n",
    "    \n",
    "    def generate_word(self, context):\n",
    "        \"\"\"Generate the next word as the most probable word given the context.\"\"\"\n",
    "        dist = self._generate_distribution(context)\n",
    "        return max(dist, key=lambda x: x[1])[0]\n",
    "        \n",
    "    def sample_word(self, context):\n",
    "        \"\"\"Generate the next word by sampling from the distribution of the next word given the context.\"\"\"\n",
    "        dist = self._generate_distribution(context)\n",
    "        x = random.random()\n",
    "        for word, prob in dist:\n",
    "            x -= prob\n",
    "            if x <= 0:\n",
    "                break\n",
    "        return word\n",
    "            \n",
    "    def _log_probability_of_sequence(self, sequence):\n",
    "        \"\"\"Calculate the probability of a sequence using the chain rule of probability and a bigram markov assumption.\n",
    "        \n",
    "        This calculation is done in log space to prevent underflow.\n",
    "        \"\"\"\n",
    "        sequence_score = 0\n",
    "        context = None\n",
    "        for word in sequence:\n",
    "            sequence_score += log(self.generate_probability(word, context))\n",
    "            context = word\n",
    "        return sequence_score\n",
    "    \n",
    "    def probability_of_sequence(self, sequence):\n",
    "        \"\"\"Convert the log probability of a sequence to the normal probability.\"\"\"\n",
    "        return exp(self._log_probability_of_sequence(sequence))\n",
    "    \n",
    "    def perplexity(self, test_sequence):\n",
    "        \"\"\"Calculate the perplexity of a sequence.\n",
    "        \n",
    "        This calculation is done in log space to prevent underflow.\n",
    "        \"\"\"\n",
    "        test_set_log_prob = self._log_probability_of_sequence(test_sequence)\n",
    "        perplexity = exp(-1/len(test_sequence) * test_set_log_prob)\n",
    "        return perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = read_data(\"data/train.txt\")\n",
    "dev_data = read_data(\"data/dev.txt\")\n",
    "test_data = read_data(\"data/test.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm = LanguageModel(train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Probability of a word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The probability of the word \"the\" is: 0.056568098761030396\n",
      "The probability of the word \"tucan\" is: 1.1141812995810679e-06\n",
      "The probability of the word \"<unk>\" given \"the\" is: 0.06365087462357452\n",
      "The probability of the word \"said\" given \"the\" is: 1.645575869275453e-05\n",
      "The probability of the word \"said\" given \"he\" is: 0.039909030885481624\n"
     ]
    }
   ],
   "source": [
    "print('The probability of the word \"the\" is: {}'.format(lm.generate_probability(\"the\")))\n",
    "print('The probability of the word \"tucan\" is: {}'.format(lm.generate_probability(\"tucan\")))\n",
    "print('The probability of the word \"<unk>\" given \"the\" is: {}'.format(lm.generate_probability(\"<unk>\", \"the\")))\n",
    "print('The probability of the word \"said\" given \"the\" is: {}'.format(lm.generate_probability(\"said\", \"the\")))\n",
    "print('The probability of the word \"said\" given \"he\" is: {}'.format(lm.generate_probability(\"said\", \"he\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Probability of a sequence\n",
    "\n",
    "Pulling a random sentence from the dev set we calculate the probability of that sequence. The language model should assign the actual sentence a higher probability than a shuffled version of the sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probability of \"issues and the new syndicated reality show hard copy records viewers ' opinions for possible airing on the next day 's\": 1.5723075859672447e-70\n",
      "Probability of \"viewers show reality records possible airing for on and hard issues day the syndicated 's ' opinions copy next the new\": 2.801304066479877e-82\n"
     ]
    }
   ],
   "source": [
    "sentence_length = random.randint(10, 21)\n",
    "start_index = random.randint(50, 65)\n",
    "sentence = dev_data[start_index:start_index + sentence_length]\n",
    "sentence_string = ' '.join(sentence)\n",
    "\n",
    "ordered_probability = lm.probability_of_sequence(sentence)\n",
    "print('Probability of \"{}\": {}'.format(sentence_string, ordered_probability))\n",
    "\n",
    "random.shuffle(sentence)\n",
    "shuffled_string = ' '.join(sentence)\n",
    "shuffled_probability = lm.probability_of_sequence(sentence)\n",
    "print('Probability of \"{}\": {}'.format(shuffled_string, shuffled_probability))\n",
    "assert ordered_probability > shuffled_probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The test set perplexity is 960.9778236067062\n"
     ]
    }
   ],
   "source": [
    "ppl = lm.perplexity(test_data)\n",
    "print(\"The test set perplexity is {}\".format(ppl))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The preprocessed PTB has several words appended to the start of the train.txt so that the \n",
    "vocabulary of the train and test match. This is the only time those words appear so using\n",
    "one as the context should generate the next one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'banknote'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm.generate_word(\"aer\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generating the next word with\n",
    "$$\\DeclareMathOperator*{\\argmax}{argmax} \\argmax_{w \\in V} P(w | w_{i-1})$$\n",
    "looks bad with this dataset because the most probable word in \"the\", and the most probable word givein \"the\" is \"&lt;unk&gt;\". The most probable word givin \"&lt;unk&gt;\" is \"&lt;unk&gt;\" which means generated sentences all end up as \"the &lt;unk&gt; &lt;unk&gt; &lt;unk&gt; ...\"\n",
    "\n",
    "To fix this we could not generate \"&lt;unk&gt;\" and take the second most probably word if the most probable is \"&lt;unk&gt;\" or we can sample from the the distributions of words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The most probable word given \"the\" is: <unk>\n",
      "Sampling from the distribution given \"the\" is: very\n"
     ]
    }
   ],
   "source": [
    "print('The most probable word given \"the\" is: {}'.format(lm.generate_word(\"the\")))\n",
    "print('Sampling from the distribution given \"the\" is: {}'.format(lm.sample_word(\"the\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
